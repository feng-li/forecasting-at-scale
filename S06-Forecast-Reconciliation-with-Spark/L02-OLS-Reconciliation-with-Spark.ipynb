{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# OLS Reconciliation with Spark\n",
    "\n",
    "## Feng Li\n",
    "\n",
    "### Guanghua School of Management\n",
    "### Peking University\n",
    "\n",
    "\n",
    "### [feng.li@gsm.pku.edu.cn](feng.li@gsm.pku.edu.cn)\n",
    "### Course home page: [https://feng.li/bdcf](https://feng.li/bdcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 14:23:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"None\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3b9487a5a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys # Ensure All environment variables are properly set \n",
    "# os.environ[\"JAVA_HOME\"] = os.path.dirname(sys.executable)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession # build Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.ui.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.cores.max\", \"32\") \\\n",
    "    .config(\"spark.driver.memory\", \"30g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"96\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"8\") \\\n",
    "    .appName(\"Spark Forecasting\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_sdf = spark.read.csv(\"../data/tourism/tourism_train.csv\", header=True, inferSchema=True)\n",
    "test_sdf = spark.read.csv(\"../data/tourism/tourism_test.csv\", header=True, inferSchema=True)\n",
    "forecast_sdf = spark.read.csv(\"../data/tourism/ets_forecasts.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|      date|Region_Category|          Visitors|\n",
      "+----------+---------------+------------------+\n",
      "|1998-01-01|       TotalAll|45151.071280099975|\n",
      "|1998-01-01|           AAll|17515.502379600006|\n",
      "|1998-01-01|           BAll|10393.618015699998|\n",
      "|1998-01-01|           CAll| 8633.359046599999|\n",
      "|1998-01-01|           DAll|3504.3133462000005|\n",
      "|1998-01-01|           EAll|      3121.6191894|\n",
      "|1998-01-01|           FAll|1850.7357734999998|\n",
      "|1998-01-01|           GAll|131.92352909999997|\n",
      "|1998-01-01|          AAAll|      4977.2096105|\n",
      "|1998-01-01|          ABAll| 5322.738721600001|\n",
      "|1998-01-01|          ACAll|3569.6213724000004|\n",
      "|1998-01-01|          ADAll|      1472.9706096|\n",
      "|1998-01-01|          AEAll|      1560.5142545|\n",
      "|1998-01-01|          AFAll|        612.447811|\n",
      "|1998-01-01|          BAAll|       3854.672582|\n",
      "|1998-01-01|          BBAll|1653.9957826000002|\n",
      "|1998-01-01|          BCAll|      2138.7473162|\n",
      "|1998-01-01|          BDAll|      1395.3775834|\n",
      "|1998-01-01|          BEAll|1350.8247515000003|\n",
      "|1998-01-01|          CAAll| 6421.236419000001|\n",
      "+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|      date|Region_Category|          Visitors|\n",
      "+----------+---------------+------------------+\n",
      "|2015-12-01|       TotalAll|24982.024449599998|\n",
      "|2015-12-01|           AAll|      7166.1237555|\n",
      "|2015-12-01|           BAll|      5340.1512778|\n",
      "|2015-12-01|           CAll| 5621.323498100002|\n",
      "|2015-12-01|           DAll|1871.7627924999997|\n",
      "|2015-12-01|           EAll|      3662.9183909|\n",
      "|2015-12-01|           FAll| 903.1237441000002|\n",
      "|2015-12-01|           GAll|416.62099070000016|\n",
      "|2015-12-01|          AAAll|2107.3938028000007|\n",
      "|2015-12-01|          ABAll|      2084.6943991|\n",
      "|2015-12-01|          ACAll|       829.2701471|\n",
      "|2015-12-01|          ADAll| 838.6748597999999|\n",
      "|2015-12-01|          AEAll|       760.0565663|\n",
      "|2015-12-01|          AFAll|       546.0339804|\n",
      "|2015-12-01|          BAAll|2659.1528845999997|\n",
      "|2015-12-01|          BBAll|       645.4208704|\n",
      "|2015-12-01|          BCAll|       576.0730749|\n",
      "|2015-12-01|          BDAll| 768.1817073999999|\n",
      "|2015-12-01|          BEAll|       691.3227405|\n",
      "|2015-12-01|          CAAll|3101.1905463999997|\n",
      "+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|      date|Region_Category|          Forecast|\n",
      "+----------+---------------+------------------+\n",
      "|2015-12-01|         AAAAll| 2058.838101212888|\n",
      "|2016-01-01|         AAAAll|3162.9085270260384|\n",
      "|2016-02-01|         AAAAll|1744.1909768938476|\n",
      "|2016-03-01|         AAAAll|2059.3010229302345|\n",
      "|2016-04-01|         AAAAll|  2060.36170915585|\n",
      "|2016-05-01|         AAAAll| 1972.482680954841|\n",
      "|2016-06-01|         AAAAll| 1846.108381522378|\n",
      "|2016-07-01|         AAAAll| 2151.959971338432|\n",
      "|2016-08-01|         AAAAll| 1872.768734964083|\n",
      "|2016-09-01|         AAAAll|2014.0112033543805|\n",
      "|2016-10-01|         AAAAll|2296.4055573391643|\n",
      "|2016-11-01|         AAAAll| 2043.693618904705|\n",
      "|2015-12-01|         AAABus|455.55263433165527|\n",
      "|2016-01-01|         AAABus| 296.1898590727801|\n",
      "|2016-02-01|         AAABus| 453.3714726155002|\n",
      "|2016-03-01|         AAABus|  525.339287022726|\n",
      "|2016-04-01|         AAABus| 459.2040763240983|\n",
      "|2016-05-01|         AAABus| 554.7602408402748|\n",
      "|2016-06-01|         AAABus| 498.8249232185846|\n",
      "|2016-07-01|         AAABus| 604.1654402560752|\n",
      "+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forecast_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# If you want to have alternative forecasts, change the following code to obtain `forecast_sdf`\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, DateType\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Define schema for the forecast output\n",
    "forecast_schema = StructType([\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"Region_Category\", StringType(), False),\n",
    "    StructField(\"Forecast\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "def ets_forecast(pdf):\n",
    "    \"\"\"Fits an ETS model for a single region and forecasts 12 months ahead.\"\"\"\n",
    "    region = pdf[\"Region_Category\"].iloc[0]  # Extract region name\n",
    "    pdf = pdf.sort_values(\"date\")  # Ensure time series is sorted\n",
    "\n",
    "    try:\n",
    "        # Drop missing values\n",
    "        ts = pdf[\"Visitors\"].dropna()\n",
    "\n",
    "        if len(ts) >= 24:  # Ensure at least 24 observations\n",
    "            model = ExponentialSmoothing(ts, trend=\"add\", seasonal=\"add\", seasonal_periods=12)\n",
    "            fitted_model = model.fit()\n",
    "            forecast = fitted_model.forecast(steps=12)\n",
    "        else:\n",
    "            forecast = [None] * 12  # Not enough data\n",
    "    except:\n",
    "        forecast = [None] * 12  # Handle errors\n",
    "\n",
    "    # Adjust forecast dates to start of the month\n",
    "    last_date = pdf[\"date\"].max()\n",
    "    future_dates = pd.date_range(start=last_date, periods=12, freq=\"ME\") + MonthBegin(1)\n",
    "\n",
    "    # Return results as a DataFrame\n",
    "    return pd.DataFrame({\"date\": future_dates, \"Region_Category\": region, \"Forecast\": forecast})\n",
    "\n",
    "# Apply the ETS model in parallel using applyInPandas\n",
    "forecast_sdf = train_sdf.groupBy(\"Region_Category\").applyInPandas(ets_forecast, schema=forecast_schema)\n",
    "\n",
    "# Show forecasted results\n",
    "forecast_sdf.show()\n",
    "\n",
    "# Save forecasts if needed\n",
    "forecast_sdf.write.csv(os.path.expanduser(\"~/ets_forecasts.csv\"), header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MinT-OLS approximation\n",
    "\n",
    "Since PySpark doesn't support matrix operations like NumPy, we'll use the **MinT-OLS approximation**, which assumes the forecast error covariance matrix \\( W = I \\) (identity matrix). This simplifies the formula:\n",
    "\n",
    "$$\n",
    "\\tilde{y} = S (S^\\top S)^{-1} S^\\top \\hat{y}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- `forecast_sdf` contains **base forecasts** for each `Region_Category` and `date`.\n",
    "- `test_sdf` is your **test set** with actual `Visitors` by `Region_Category` and `date`.\n",
    "- You have `summing_sdf_long` with:  \n",
    "  - `Parent_Group`  \n",
    "  - `Region_Category`  \n",
    "  - `Weight` (usually 0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/25 14:59:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+------+\n",
      "|Parent_Group|Region_Category|Weight|\n",
      "+------------+---------------+------+\n",
      "|    TotalAll|         AAAHol|   1.0|\n",
      "|    TotalAll|         AAAVis|   1.0|\n",
      "|    TotalAll|         AAABus|   1.0|\n",
      "|    TotalAll|         AAAOth|   1.0|\n",
      "|    TotalAll|         AABHol|   1.0|\n",
      "|    TotalAll|         AABVis|   1.0|\n",
      "|    TotalAll|         AABBus|   1.0|\n",
      "|    TotalAll|         AABOth|   1.0|\n",
      "|    TotalAll|         ABAHol|   1.0|\n",
      "|    TotalAll|         ABAVis|   1.0|\n",
      "|    TotalAll|         ABABus|   1.0|\n",
      "|    TotalAll|         ABAOth|   1.0|\n",
      "|    TotalAll|         ABBHol|   1.0|\n",
      "|    TotalAll|         ABBVis|   1.0|\n",
      "|    TotalAll|         ABBBus|   1.0|\n",
      "|    TotalAll|         ABBOth|   1.0|\n",
      "|    TotalAll|         ACAHol|   1.0|\n",
      "|    TotalAll|         ACAVis|   1.0|\n",
      "|    TotalAll|         ACABus|   1.0|\n",
      "|    TotalAll|         ACAOth|   1.0|\n",
      "+------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "\n",
    "# Load the summing matrix file\n",
    "summing_matrix_path = \"../data/tourism/agg_mat.csv\"  # Update with actual path\n",
    "\n",
    "# Load the summing matrix file (skip the first column)\n",
    "summing_sdf = spark.read.csv(summing_matrix_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Convert from wide format to long format (Region_Category, Parent_Group, Weight)\n",
    "summing_sdf_long = summing_sdf.selectExpr(\n",
    "    \"Parent_Group\",\n",
    "    \"stack(\" + str(len(summing_sdf.columns) - 1) + \", \" +\n",
    "    \", \".join([f\"'{col}', {col}\" for col in summing_sdf.columns if col != \"Parent_Group\"]) +\n",
    "    \") as (Region_Category, Weight)\"\n",
    ")\n",
    "\n",
    "# Show the reshaped summing matrix\n",
    "summing_sdf_long.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------------+\n",
      "|      date|Parent_Group|Reconciled_Forecast|\n",
      "+----------+------------+-------------------+\n",
      "|2015-12-01|      DBBHol|  93.30359324977445|\n",
      "|2015-12-01|      BEDBus|  9.587146785063556|\n",
      "|2015-12-01|      BEAHol|  5.495956927738096|\n",
      "|2015-12-01|      ADBBus|  33.48580576600097|\n",
      "|2015-12-01|      ADBHol|  67.11613382657367|\n",
      "|2015-12-01|       FCVis|  49.72621582615672|\n",
      "|2015-12-01|       EABus|  337.9295021569872|\n",
      "|2015-12-01|       ACBus|  72.84836998814711|\n",
      "|2015-12-01|      GACAll| 25.191637997601127|\n",
      "|2016-01-01|      DCBHol|  122.5716884443587|\n",
      "|2016-01-01|      BBAHol| 1215.3210703251198|\n",
      "|2016-01-01|      AFAHol| 231.28278439478635|\n",
      "|2016-01-01|      ABAVis|  444.3775568094911|\n",
      "|2016-01-01|      ABAHol|  738.9134710470801|\n",
      "|2016-01-01|       GAVis|   93.9877077354365|\n",
      "|2016-01-01|       BEHol|  499.4794722311132|\n",
      "|2016-01-01|       AABus| 324.74260821819576|\n",
      "|2016-01-01|      DABAll|  53.60525695425169|\n",
      "|2016-01-01|        GAll| 360.19255578422724|\n",
      "|2016-02-01|      GAABus|  84.29362260554478|\n",
      "+----------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mint_input_sdf = forecast_sdf.join(summing_sdf_long, on=\"Region_Category\", how=\"inner\")\n",
    "\n",
    "# Multiply base forecast by structure weights\n",
    "mint_input_sdf = mint_input_sdf.withColumn(\"Weighted_Forecast\", col(\"Forecast\") * col(\"Weight\"))\n",
    "\n",
    "# Sum up forecasts to each parent group\n",
    "mint_reconciled_sdf = mint_input_sdf.groupBy(\"date\", \"Parent_Group\").agg(\n",
    "    spark_sum(\"Weighted_Forecast\").alias(\"Reconciled_Forecast\")\n",
    ")\n",
    "mint_reconciled_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Parent_Group|              MAPE|\n",
      "+------------+------------------+\n",
      "|      CBDAll|0.9918551321816805|\n",
      "|       BCHol|0.9857934130236617|\n",
      "|      BCBOth|0.9998273964952409|\n",
      "|      DDBHol|0.9965713950479036|\n",
      "|      CCBAll|0.9872982701563636|\n",
      "|       CCOth|0.9971415937249763|\n",
      "|      DCCAll|0.9987860340371489|\n",
      "|      BDEAll|0.9986953765843504|\n",
      "|      FBAVis|0.9996472874910226|\n",
      "|      EABVis|0.9849344998635591|\n",
      "|      GABVis|0.9997748506792165|\n",
      "|      ADBAll|0.9903498750139902|\n",
      "|      FAAHol|0.9929209612350367|\n",
      "|      BDFAll|0.9989229459704517|\n",
      "|      CBCHol| 0.996186434565634|\n",
      "|      GBCAll|0.9974573023722869|\n",
      "|      CDBHol| 0.998463935084155|\n",
      "|      BEGAll|0.9955394793307324|\n",
      "|       DABus|0.9938826685744151|\n",
      "|      DAAVis|0.9899752829805119|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_parent_sdf = test_sdf.join(summing_sdf_long, on=\"Region_Category\", how=\"inner\") \\\n",
    "    .groupBy(\"date\", \"Parent_Group\") \\\n",
    "    .agg(spark_sum(\"Visitors\").alias(\"Actual_Visitors\"))\n",
    "\n",
    "from pyspark.sql.functions import abs, mean\n",
    "\n",
    "evaluation_sdf = mint_reconciled_sdf.join(test_parent_sdf, on=[\"date\", \"Parent_Group\"], how=\"inner\") \\\n",
    "    .withColumn(\"APE\", abs((col(\"Reconciled_Forecast\") - col(\"Actual_Visitors\")) / col(\"Actual_Visitors\")))\n",
    "\n",
    "mape_sdf = evaluation_sdf.groupBy(\"Parent_Group\").agg(mean(\"APE\").alias(\"MAPE\"))\n",
    "mape_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      Overall_MAPE|\n",
      "+------------------+\n",
      "|0.9860567355464035|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute overall mean MAPE across all Region_Category\n",
    "overall_mape_ols = mape_sdf.agg(mean(\"MAPE\").alias(\"Overall_MAPE\"))\n",
    "\n",
    "# Show the result\n",
    "overall_mape_ols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of MinT-OLS\n",
    "\n",
    "- **MinT-OLS (simple projection)**  \n",
    "- **No covariance matrix needed**  \n",
    "- **Coherent forecasts at Parent_Group level**  \n",
    "- **Evaluated using MAPE**\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
