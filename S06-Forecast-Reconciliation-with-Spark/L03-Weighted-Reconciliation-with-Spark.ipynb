{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Weighted Reconciliation with Spark\n",
    "\n",
    "## Feng Li\n",
    "\n",
    "### Guanghua School of Management\n",
    "### Peking University\n",
    "\n",
    "\n",
    "### [feng.li@gsm.pku.edu.cn](feng.li@gsm.pku.edu.cn)\n",
    "### Course home page: [https://feng.li/bdcf](https://feng.li/bdcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MinT-WLS\n",
    "\n",
    "\n",
    "- MinT-WLS assumes a **diagonal forecast error covariance matrix** $ W $, where each diagonal element is the **variance of forecast errors** for each series (e.g., Region_Category). The formula becomes:\n",
    "\n",
    "$$\n",
    "\\tilde{y} = S (S^\\top W^{-1} S)^{-1} S^\\top W^{-1} \\hat{y}\n",
    "$$\n",
    "\n",
    "- $ \\hat{y} $: base forecasts (from ETS, etc.)\n",
    "- $ S $: summing matrix\n",
    "- $ W $: diagonal matrix of **forecast error variances** from the training residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to Approximate MinT-WLS in Spark?\n",
    "\n",
    "Since Spark is not optimized for full matrix ops, you can:\n",
    "\n",
    "- Estimate **error variance per Region_Category** using training residuals.\n",
    "- To compute forecast error variances for use in MinT-WLS, you need **in-sample forecasts** (i.e., forecasts over the training period, not just for the future 12 months). These are often called fitted values from the model.\n",
    "- Use the **inverse variance** as weights.\n",
    "- Perform a **weighted projection** manually (just like MinT-OLS, but with weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 15:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"None\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd10414ade0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys # Ensure All environment variables are properly set \n",
    "# os.environ[\"JAVA_HOME\"] = os.path.dirname(sys.executable)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession # build Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.ui.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.cores.max\", \"32\") \\\n",
    "    .config(\"spark.driver.memory\", \"30g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"96\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"8\") \\\n",
    "    .appName(\"Spark Forecasting\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_sdf = spark.read.csv(\"../data/tourism/tourism_train.csv\", header=True, inferSchema=True)\n",
    "test_sdf = spark.read.csv(\"../data/tourism/tourism_test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark approach\n",
    "\n",
    "- You can modify the `ets_forecast` function to return both the **fitted values** (in-sample) and **forecast values** (out-of-sample) in a single column, while tagging them with a type column (\"fitted\" or \"forecast\"). \n",
    "- Later, you can easily split or filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Modified ets_forecast Function: Fitted + Forecast in One UDF\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import pandas as pd\n",
    "\n",
    "# Schema with an extra 'type' column to label fitted vs forecast\n",
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"Region_Category\", StringType(), False),\n",
    "    StructField(\"Forecast\", DoubleType(), False),\n",
    "    StructField(\"type\", StringType(), False)\n",
    "])\n",
    "\n",
    "def ets_fitted_and_forecast(pdf):\n",
    "    region = pdf[\"Region_Category\"].iloc[0]\n",
    "    pdf = pdf.sort_values(\"date\")\n",
    "\n",
    "    try:\n",
    "        ts = pdf[\"Visitors\"].dropna()\n",
    "        dates = pdf[\"date\"]\n",
    "\n",
    "        if len(ts) >= 24:\n",
    "            model = ExponentialSmoothing(ts, trend=\"add\", seasonal=\"add\", seasonal_periods=12)\n",
    "            fitted_model = model.fit()\n",
    "\n",
    "            # Fitted values (same length as training data)\n",
    "            fitted_values = fitted_model.fittedvalues\n",
    "            fitted_df = pd.DataFrame({\n",
    "                \"date\": dates[-len(fitted_values):].values,\n",
    "                \"Region_Category\": region,\n",
    "                \"Forecast\": fitted_values.values,\n",
    "                \"type\": \"fitted\"\n",
    "            })\n",
    "\n",
    "            # Forecast for next 12 months\n",
    "            forecast_values = fitted_model.forecast(steps=12)\n",
    "            last_date = pdf[\"date\"].max()\n",
    "            forecast_dates = pd.date_range(start=last_date, periods=12, freq=\"ME\") + MonthBegin(1)\n",
    "\n",
    "            forecast_df = pd.DataFrame({\n",
    "                \"date\": forecast_dates,\n",
    "                \"Region_Category\": region,\n",
    "                \"Forecast\": forecast_values.values,\n",
    "                \"type\": \"forecast\"\n",
    "            })\n",
    "\n",
    "            result = pd.concat([fitted_df, forecast_df], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            result = pd.DataFrame({\n",
    "                \"date\": pdf[\"date\"],\n",
    "                \"Region_Category\": region,\n",
    "                \"Forecast\": [None] * len(pdf),\n",
    "                \"type\": \"fitted\"\n",
    "            })\n",
    "\n",
    "    except:\n",
    "        result = pd.DataFrame({\n",
    "            \"date\": pdf[\"date\"],\n",
    "            \"Region_Category\": region,\n",
    "            \"Forecast\": [None] * len(pdf),\n",
    "            \"type\": \"fitted\"\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+------+\n",
      "|      date|Region_Category|          Forecast|  type|\n",
      "+----------+---------------+------------------+------+\n",
      "|1998-01-01|         AAAAll|3246.4056861089557|fitted|\n",
      "|1998-02-01|         AAAAll|1846.3514350015978|fitted|\n",
      "|1998-03-01|         AAAAll| 2013.827299893829|fitted|\n",
      "|1998-04-01|         AAAAll| 2312.356250241453|fitted|\n",
      "|1998-05-01|         AAAAll| 1976.696739197145|fitted|\n",
      "|1998-06-01|         AAAAll|1886.1704427003526|fitted|\n",
      "|1998-07-01|         AAAAll|2252.3437065440035|fitted|\n",
      "|1998-08-01|         AAAAll|1997.6286143492266|fitted|\n",
      "|1998-09-01|         AAAAll|2190.5384498174626|fitted|\n",
      "|1998-10-01|         AAAAll| 2422.643967592123|fitted|\n",
      "|1998-11-01|         AAAAll|2099.5706305548833|fitted|\n",
      "|1998-12-01|         AAAAll| 2158.740350374741|fitted|\n",
      "|1999-01-01|         AAAAll|3193.7236309368136|fitted|\n",
      "|1999-02-01|         AAAAll|1617.9434116495956|fitted|\n",
      "|1999-03-01|         AAAAll| 1779.115328477587|fitted|\n",
      "|1999-04-01|         AAAAll|2130.6088706344285|fitted|\n",
      "|1999-05-01|         AAAAll|1897.6403231005363|fitted|\n",
      "|1999-06-01|         AAAAll|  1712.39204850544|fitted|\n",
      "|1999-07-01|         AAAAll| 2083.393284857496|fitted|\n",
      "|1999-08-01|         AAAAll|1790.7158605841455|fitted|\n",
      "+----------+---------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "forecast_all_sdf = train_sdf.groupBy(\"Region_Category\").applyInPandas(\n",
    "    ets_fitted_and_forecast,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "forecast_all_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+------+\n",
      "|      date|Region_Category|          Forecast|  type|\n",
      "+----------+---------------+------------------+------+\n",
      "|1998-01-01|         AAAAll|3246.4056861089557|fitted|\n",
      "|1998-02-01|         AAAAll|1846.3514350015978|fitted|\n",
      "|1998-03-01|         AAAAll| 2013.827299893829|fitted|\n",
      "|1998-04-01|         AAAAll| 2312.356250241453|fitted|\n",
      "|1998-05-01|         AAAAll| 1976.696739197145|fitted|\n",
      "|1998-06-01|         AAAAll|1886.1704427003526|fitted|\n",
      "|1998-07-01|         AAAAll|2252.3437065440035|fitted|\n",
      "|1998-08-01|         AAAAll|1997.6286143492266|fitted|\n",
      "|1998-09-01|         AAAAll|2190.5384498174626|fitted|\n",
      "|1998-10-01|         AAAAll| 2422.643967592123|fitted|\n",
      "|1998-11-01|         AAAAll|2099.5706305548833|fitted|\n",
      "|1998-12-01|         AAAAll| 2158.740350374741|fitted|\n",
      "|1999-01-01|         AAAAll|3193.7236309368136|fitted|\n",
      "|1999-02-01|         AAAAll|1617.9434116495956|fitted|\n",
      "|1999-03-01|         AAAAll| 1779.115328477587|fitted|\n",
      "|1999-04-01|         AAAAll|2130.6088706344285|fitted|\n",
      "|1999-05-01|         AAAAll|1897.6403231005363|fitted|\n",
      "|1999-06-01|         AAAAll|  1712.39204850544|fitted|\n",
      "|1999-07-01|         AAAAll| 2083.393284857496|fitted|\n",
      "|1999-08-01|         AAAAll|1790.7158605841455|fitted|\n",
      "+----------+---------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+--------+\n",
      "|      date|Region_Category|          Forecast|    type|\n",
      "+----------+---------------+------------------+--------+\n",
      "|2015-12-01|         AAAAll| 2058.838101212888|forecast|\n",
      "|2016-01-01|         AAAAll|3162.9085270260384|forecast|\n",
      "|2016-02-01|         AAAAll|1744.1909768938476|forecast|\n",
      "|2016-03-01|         AAAAll|2059.3010229302345|forecast|\n",
      "|2016-04-01|         AAAAll|  2060.36170915585|forecast|\n",
      "|2016-05-01|         AAAAll| 1972.482680954841|forecast|\n",
      "|2016-06-01|         AAAAll| 1846.108381522378|forecast|\n",
      "|2016-07-01|         AAAAll| 2151.959971338432|forecast|\n",
      "|2016-08-01|         AAAAll| 1872.768734964083|forecast|\n",
      "|2016-09-01|         AAAAll|2014.0112033543805|forecast|\n",
      "|2016-10-01|         AAAAll|2296.4055573391643|forecast|\n",
      "|2016-11-01|         AAAAll| 2043.693618904705|forecast|\n",
      "|2015-12-01|         AAABus|455.55263433165527|forecast|\n",
      "|2016-01-01|         AAABus| 296.1898590727801|forecast|\n",
      "|2016-02-01|         AAABus| 453.3714726155002|forecast|\n",
      "|2016-03-01|         AAABus|  525.339287022726|forecast|\n",
      "|2016-04-01|         AAABus| 459.2040763240983|forecast|\n",
      "|2016-05-01|         AAABus| 554.7602408402748|forecast|\n",
      "|2016-06-01|         AAABus| 498.8249232185846|forecast|\n",
      "|2016-07-01|         AAABus| 604.1654402560752|forecast|\n",
      "+----------+---------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "train_forecast_sdf = forecast_all_sdf.filter(col(\"type\") == \"fitted\")\n",
    "forecast_sdf = forecast_all_sdf.filter(col(\"type\") == \"forecast\")\n",
    "\n",
    "train_forecast_sdf.show()\n",
    "forecast_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pow, avg\n",
    "\n",
    "# Extract In-Sample Residuals and Estimate Variance\n",
    "\n",
    "# Step 1: Get fitted values only\n",
    "fitted_sdf = forecast_all_sdf.filter(col(\"type\") == \"fitted\")\n",
    "\n",
    "# Step 2: Join with actual visitors from train_sdf to compute residuals\n",
    "residuals_sdf = fitted_sdf.join(train_sdf, on=[\"date\", \"Region_Category\"], how=\"inner\") \\\n",
    "    .withColumn(\"squared_error\", pow(col(\"Forecast\") - col(\"Visitors\"), 2))\n",
    "\n",
    "# Step 3: Compute variance per Region_Category (mean squared error)\n",
    "error_variance_sdf = residuals_sdf.groupBy(\"Region_Category\").agg(\n",
    "    avg(\"squared_error\").alias(\"Forecast_Variance\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "25/03/25 15:49:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+------+\n",
      "|Parent_Group|Region_Category|Weight|\n",
      "+------------+---------------+------+\n",
      "|    TotalAll|         AAAHol|   1.0|\n",
      "|    TotalAll|         AAAVis|   1.0|\n",
      "|    TotalAll|         AAABus|   1.0|\n",
      "|    TotalAll|         AAAOth|   1.0|\n",
      "|    TotalAll|         AABHol|   1.0|\n",
      "|    TotalAll|         AABVis|   1.0|\n",
      "|    TotalAll|         AABBus|   1.0|\n",
      "|    TotalAll|         AABOth|   1.0|\n",
      "|    TotalAll|         ABAHol|   1.0|\n",
      "|    TotalAll|         ABAVis|   1.0|\n",
      "|    TotalAll|         ABABus|   1.0|\n",
      "|    TotalAll|         ABAOth|   1.0|\n",
      "|    TotalAll|         ABBHol|   1.0|\n",
      "|    TotalAll|         ABBVis|   1.0|\n",
      "|    TotalAll|         ABBBus|   1.0|\n",
      "|    TotalAll|         ABBOth|   1.0|\n",
      "|    TotalAll|         ACAHol|   1.0|\n",
      "|    TotalAll|         ACAVis|   1.0|\n",
      "|    TotalAll|         ACABus|   1.0|\n",
      "|    TotalAll|         ACAOth|   1.0|\n",
      "+------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "\n",
    "# Load the summing matrix file\n",
    "summing_matrix_path = \"../data/tourism/agg_mat.csv\"  # Update with actual path\n",
    "\n",
    "# Load the summing matrix file (skip the first column)\n",
    "summing_sdf = spark.read.csv(summing_matrix_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Convert from wide format to long format (Region_Category, Parent_Group, Weight)\n",
    "summing_sdf_long = summing_sdf.selectExpr(\n",
    "    \"Parent_Group\",\n",
    "    \"stack(\" + str(len(summing_sdf.columns) - 1) + \", \" +\n",
    "    \", \".join([f\"'{col}', {col}\" for col in summing_sdf.columns if col != \"Parent_Group\"]) +\n",
    "    \") as (Region_Category, Weight)\"\n",
    ")\n",
    "\n",
    "# Show the reshaped summing matrix\n",
    "summing_sdf_long.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Extract 12-step forecasts\n",
    "forecast_sdf = forecast_all_sdf.filter(col(\"type\") == \"forecast\")\n",
    "\n",
    "# Step 2: Join with summing matrix and forecast variances\n",
    "forecast_wls_sdf = forecast_sdf.join(summing_sdf_long, on=\"Region_Category\", how=\"inner\") \\\n",
    "    .join(error_variance_sdf, on=\"Region_Category\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# Step 1: Compute weighted forecast (Forecast × Weight ÷ Variance)\n",
    "forecast_wls_sdf = forecast_wls_sdf.withColumn(\n",
    "    \"Weighted_Forecast\", col(\"Forecast\") * col(\"Weight\") / col(\"Forecast_Variance\")\n",
    ").withColumn(\n",
    "    \"Normalized_Weight\", col(\"Weight\") / col(\"Forecast_Variance\")\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate numerator and denominator per Parent_Group and date\n",
    "numerator_sdf = forecast_wls_sdf.groupBy(\"date\", \"Parent_Group\").agg(\n",
    "    spark_sum(\"Weighted_Forecast\").alias(\"Numerator\"),\n",
    "    spark_sum(\"Normalized_Weight\").alias(\"Denominator\")\n",
    ")\n",
    "\n",
    "# Step 3: Compute reconciled forecast\n",
    "reconciled_wls_sdf = numerator_sdf.withColumn(\n",
    "    \"Reconciled_Forecast\", col(\"Numerator\") / col(\"Denominator\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Parent_Group|              MAPE|\n",
      "+------------+------------------+\n",
      "|      CBDAll|0.9990308809580983|\n",
      "|       BCHol|0.9952348648671636|\n",
      "|      BCBOth|0.9998273964952409|\n",
      "|      DDBHol|0.9965713950479036|\n",
      "|      CCBAll|0.9969357208629136|\n",
      "|       CCOth|0.9993691868557918|\n",
      "|      DCCAll|0.9998990568465357|\n",
      "|      BDEAll|0.9998635234555749|\n",
      "|      FBAVis|0.9996472874910226|\n",
      "|      EABVis|0.9849344998635591|\n",
      "|      GABVis|0.9997748506792165|\n",
      "|      ADBAll|0.9982988253046847|\n",
      "|      FAAHol|0.9929209612350367|\n",
      "|      BDFAll|0.9998573749089767|\n",
      "|      CBCHol| 0.996186434565634|\n",
      "|      GBCAll|0.9995320207417416|\n",
      "|      CDBHol| 0.998463935084155|\n",
      "|      BEGAll|0.9989692918469983|\n",
      "|       DABus| 0.999867619528087|\n",
      "|      DAAVis|0.9899752829805119|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "\n",
    "# Step 1: Prepare test set at Parent_Group level\n",
    "test_parent_sdf = test_sdf.join(summing_sdf_long, on=\"Region_Category\", how=\"inner\") \\\n",
    "    .groupBy(\"date\", \"Parent_Group\") \\\n",
    "    .agg(spark_sum(\"Visitors\").alias(\"Actual_Visitors\"))\n",
    "\n",
    "# Step 2: Join with reconciled forecast and compute APE\n",
    "evaluation_sdf = reconciled_wls_sdf.join(test_parent_sdf, on=[\"date\", \"Parent_Group\"], how=\"inner\") \\\n",
    "    .withColumn(\"APE\", abs((col(\"Reconciled_Forecast\") - col(\"Actual_Visitors\")) / col(\"Actual_Visitors\")))\n",
    "\n",
    "# Step 3: Compute MAPE per Parent_Group\n",
    "mape_sdf = evaluation_sdf.groupBy(\"Parent_Group\").agg(avg(\"APE\").alias(\"MAPE\"))\n",
    "mape_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/nfs-share/home/2406184221/.local/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      Overall_MAPE|\n",
      "+------------------+\n",
      "|0.9972887320763939|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Compute overall (mean of all Parent_Group MAPE values)\n",
    "overall_mape = mape_sdf.agg(mean(\"MAPE\").alias(\"Overall_MAPE\"))\n",
    "\n",
    "# Show the result\n",
    "overall_mape.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "- **MinT-WLS** in Spark is approximate — no matrix inversion is used. \n",
    "\n",
    "- For full matrix-based MinT with off-diagonal covariance, it requires distributed matrix inversion techniques "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
